"""Inference pipeline: prompt -> compiled gcode."""

from pathlib import Path

import torch
from diffusers import StableDiffusionPipeline

from .config import Config
from .validator import GcodeValidator


class GcodeGenerator:
    """Generates gcode from text prompts."""

    def __init__(
        self,
        model_path: str | Path | None = None,
        config: Config | None = None,
    ):
        self.config = config or Config.load()
        self.validator = GcodeValidator(self.config)
        self.model_path = model_path
        self.pipe = None

    def load_model(self):
        """Load the finetuned model."""
        if self.model_path:
            self.pipe = StableDiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
            )
        else:
            # Use base model for testing
            self.pipe = StableDiffusionPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-1-base",
                torch_dtype=torch.float16,
            )

        if torch.cuda.is_available():
            self.pipe = self.pipe.to("cuda")

    def generate(
        self,
        prompt: str,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
    ) -> str:
        """Generate gcode from prompt."""
        if self.pipe is None:
            self.load_model()

        # NOTE: Actual generation depends on model architecture
        # This skeleton shows the expected flow:
        # 1. Generate raw gcode from model
        # 2. Validate and compile for machine safety

        # Placeholder: generate gcode header with proper machine setup
        raw_gcode = self._generate_raw(prompt, num_inference_steps, guidance_scale)

        # Compile/validate for machine limits
        compiled = self.validator.compile(raw_gcode)
        return compiled

    def _generate_raw(
        self,
        prompt: str,
        num_inference_steps: int,
        guidance_scale: float,
    ) -> str:
        """Generate raw gcode (placeholder for actual model output)."""
        # This would be replaced with actual model inference
        header = f"""; Generated by dcode
; Prompt: {prompt}
; Machine: {self.config.machine.width_mm}x{self.config.machine.height_mm}mm
G28 ; Home
M280 P0 S{self.config.pen.up_angle} ; Pen up
G1 F{self.config.pen.travel_speed}
"""
        return header

