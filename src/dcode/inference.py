"""Inference: prompt â†’ gcode."""

from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer

from .config import Config
from .validator import GcodeValidator


class GcodeGenerator:
    """Generate gcode from text prompts."""

    def __init__(
        self,
        model_path: str | Path,
        config: Config | None = None,
        device: str | None = None,
    ):
        self.model_path = Path(model_path)
        self.config = config or Config.load()
        self.validator = GcodeValidator(self.config)
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        self.is_causal = False

    def load(self):
        """Load model and tokenizer."""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)

        # Detect model type from config
        model_config = self.model_path / "config.json"
        if model_config.exists():
            import json

            with open(model_config) as f:
                cfg = json.load(f)
            self.is_causal = "causal" in cfg.get("architectures", [""])[0].lower() or "gpt" in cfg.get("model_type", "").lower()

        if self.is_causal:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path, torch_dtype=torch.float16
            )
        else:
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_path, torch_dtype=torch.float16
            )

        self.model = self.model.to(self.device).eval()

    def generate(
        self,
        prompt: str,
        max_length: int = 1024,
        temperature: float = 0.8,
        top_p: float = 0.9,
        num_beams: int = 1,
    ) -> str:
        """Generate gcode from prompt."""
        if self.model is None:
            self.load()

        # Tokenize
        if self.is_causal:
            # Causal: append separator
            input_text = f"{prompt}\n###\n"
        else:
            input_text = prompt

        inputs = self.tokenizer(
            input_text, return_tensors="pt", truncation=True, max_length=128
        ).to(self.device)

        # Generate
        with torch.no_grad():
            if num_beams > 1:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                )

        # Decode
        raw = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # For causal models, extract gcode after separator
        if self.is_causal and "###" in raw:
            raw = raw.split("###", 1)[-1].strip()

        # Add header if needed
        if not raw.strip().startswith(";") and not raw.strip().startswith("G"):
            raw = self._header(prompt) + raw

        # Validate and fix
        return self.validator.compile(raw)

    def _header(self, prompt: str) -> str:
        """Gcode header."""
        return f"""; Generated by dcode
; Prompt: {prompt}
G21
G90
M280 P0 S{self.config.pen.up_angle}
G28

"""


def infer(
    prompt: str,
    model_path: str,
    output: str | None = None,
    temperature: float = 0.8,
) -> str:
    """Quick inference."""
    gen = GcodeGenerator(model_path)
    gcode = gen.generate(prompt, temperature=temperature)
    if output:
        Path(output).write_text(gcode)
    return gcode
